{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "#from soap_jax import soap\n",
    "\n",
    "import jaxmetric as jm\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "psi=0\n",
    "\n",
    "k_mod = [1]\n",
    "\n",
    "proj_facts = (4,)\n",
    "\n",
    "poly = jit(lambda x: x[0]**5 + x[1]**5 + x[2]**5 +x[3]**5 + x[4]**5 + psi* x[0]*x[1]*x[2]*x[3]*x[4])\n",
    "\n",
    "# psi0 = 0.5\n",
    "# psi1 = 1.\n",
    "\n",
    "\n",
    "# import itertools\n",
    "# combinations = list(itertools.product([0, 1], repeat=4))\n",
    "\n",
    "# @jit\n",
    "# def poly(x):\n",
    "#     X = jnp.array([x[0],x[1]])\n",
    "#     Y = jnp.array([x[2],x[3]])\n",
    "#     U = jnp.array([x[4],x[5]])\n",
    "#     V = jnp.array([x[6],x[7]])\n",
    "#     term_sum = 0\n",
    "#     term_prod = psi1 * (X[0] * Y[0] * U[0] * V[0]) * (X[1] * Y[1] * U[1] * V[1])\n",
    "#     for c in combinations:\n",
    "#         s = sum(c)\n",
    "#         term_sum += (1-s%2)*(X[c[0]] * Y[c[1]] * U[c[2]] * V[c[3]])**2. + psi0*(s%2)*(X[c[0]] * Y[c[1]] * U[c[2]] * V[c[3]])**2.\n",
    "\n",
    "#     return term_prod + term_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "# points = jm.point_gen.generate_points_calabi_yau(key, proj_facts,poly,1e5)\n",
    "\n",
    "\n",
    "# with open(\"100kPointsTQ.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(points, f)\n",
    "\n",
    "\n",
    "with open(\"100kPointsPsiEq0.pkl\", \"rb\") as f:\n",
    "    points = pickle.load(f)\n",
    "\n",
    "\n",
    "print(jnp.shape(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51561314\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = models.FuncQuintic()\n",
    "\n",
    "# Generate a PRNG key\n",
    "key = jax.random.PRNGKey(np.random.randint(0,10**6))\n",
    "\n",
    "# Initialize parameters\n",
    "params = model.init(key,  jm.complex_numbers.complex_to_real(points[0]))\n",
    "\n",
    "kappa_val = jm.metrics.kappa(proj_facts,k_mod, poly, points)\n",
    "\n",
    "print(kappa_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 5), 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split points into training and evaluation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(points) * split_ratio)\n",
    "train_points = points[:split_index]\n",
    "eval_points = points[split_index:]\n",
    "\n",
    "def batch_data(data, batch_size):\n",
    "    num_batches = len(data) // batch_size\n",
    "    return jnp.array_split(data[:num_batches * batch_size], num_batches)\n",
    "\n",
    "ptsBatched = batch_data(train_points, 1000)\n",
    "\n",
    "ptsBatched[0].shape,len(ptsBatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.11333945+1.11157615e-02j, -0.24078609+3.31749886e-01j,\n",
       "         1.        +5.72186165e-09j, -0.3029918 -9.50701594e-01j,\n",
       "         0.49222153+1.90694243e-01j],\n",
       "       [ 1.        +9.99902591e-11j, -0.35913962-8.55454087e-01j,\n",
       "         0.35343096+2.28940710e-01j,  0.9534757 -2.03216806e-01j,\n",
       "        -0.20100094+9.65161800e-01j]], dtype=complex64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptsBatched[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_grads(gradients, replacement_value=1e-8):\n",
    "    def replace(grad):\n",
    "        return jnp.where(jnp.isnan(grad), replacement_value, grad)\n",
    "    \n",
    "    return jax.tree_util.tree_map(replace, gradients)\n",
    "\n",
    "def nan_replacement_transform(replacement_value=1e-6):\n",
    "    def init_fn(params):\n",
    "        return ()\n",
    "    \n",
    "    def update_fn(updates, state, params=None):\n",
    "        sanitized_updates = jax.tree_util.tree_map(\n",
    "            lambda g: jnp.where(jnp.isnan(g), replacement_value, g),\n",
    "            updates\n",
    "        )\n",
    "        return sanitized_updates, state\n",
    "    \n",
    "    return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "optimiser = optax.chain(\n",
    "optax.clip(1.),\n",
    "#soap(learning_rate=1e-6),\n",
    "#optax.clip_by_block_rms(1e-2),\n",
    "optax.adam(learning_rate=1e-2),\n",
    "nan_replacement_transform(1e-8)\n",
    ")\n",
    "\n",
    "my_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimiser)\n",
    "\n",
    "grad_func = jax.jit(jax.value_and_grad(jm.losses.loss_ma,argnums=1),static_argnums=(0,2,4))\n",
    "\n",
    "# Define the training step\n",
    "@jax.jit\n",
    "def train_step(state,x):\n",
    "  loss, grads = grad_func(model, state.params,proj_facts,k_mod, poly,  kappa_val, x)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state, loss\n",
    "\n",
    "@jax.jit\n",
    "def evaluate_step(state,x):\n",
    "  return jm.losses.loss_ma(model, state.params,proj_facts,k_mod, poly, kappa_val, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last Eval Loss 0.42648845911026 | Batch 52/81  | Batch Loss: 0.4138628840446472:   6%|â–Œ         | 6/100 [01:08<17:50, 11.38s/it]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m order:\n\u001b[1;32m     14\u001b[0m   k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 15\u001b[0m   my_state, l \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptsBatched\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast Eval Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28meval\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatchNum\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  | Batch Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     17\u001b[0m   loss\u001b[38;5;241m.\u001b[39mappend(l)\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "eval = evaluate_step(my_state,eval_points)\n",
    "loss = []\n",
    "lossEval = []\n",
    "pbar = tqdm(range(100))\n",
    "batchNum = len(ptsBatched)\n",
    "pbar.set_description(f\"Last Eval Loss {eval} | Batch {0}/{batchNum+1}  | Batch Loss: NA\")\n",
    "\n",
    "for i in pbar:\n",
    "  order = np.array(range(batchNum))\n",
    "  np.random.shuffle(order)\n",
    "  k=1\n",
    "  for j in order:\n",
    "    k+=1\n",
    "    my_state, l = train_step(my_state, ptsBatched[j])\n",
    "    pbar.set_description(f\"Last Eval Loss {eval} | Batch {k}/{batchNum+1}  | Batch Loss: {l}\" )\n",
    "    loss.append(l)\n",
    "  eval = evaluate_step(my_state,eval_points)\n",
    "  lossEval.append([(i+1)*batchNum,eval])\n",
    "\n",
    "lossEval = np.array(lossEval)\n",
    "loss = jnp.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossEval=jnp.array(lossEval)\n",
    "loss = jnp.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((loss[2:]))\n",
    "plt.plot(lossEval[1:,0],(lossEval[1:,1]))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxVenv",
   "language": "python",
   "name": "jaxvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
